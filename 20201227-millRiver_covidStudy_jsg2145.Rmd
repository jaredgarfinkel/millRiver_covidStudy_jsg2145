---
title: "20201227-millRiver_covidStudy_jsg2145"
author: "Jared Garfinkel"
date: "12/27/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(readxl)
library(corrplot)
library(factoextra)
library(gridExtra)
library(psych)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 12, 
  fig.height = 14,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Part 1

## Read in data

## Keep the outcome (cases) separate

```{r}
df = read_csv("./data/P_Covid Dataset January 24 2021 - Sheet1.csv") %>% 
  janitor::clean_names() %>% 
  select(-starts_with(c("t_covid", "t_pop")), -p_not_white)

head(df)

# observe dataframe

skimr::skim(df)
```

The dataset is `r nrow(df)` rows by `r ncol(df)` columns.

There are several ways of handling missing data, including available case analysis, complete case analysis, and multiple imputation.

In this case, we will simply remove the "median rooms" variable as it is not expected to contribute meaningfully to the correlation analysis.

The cases missing "mhv" (Median Home Value) and "mhi" (Median Household Income) values are removed for the correlation analysis.

This results in a loss of efficiency of `r round(5/134, 3)*100` percent.

```{r}
data = df %>% 
  dplyr::select(-median_rooms) %>% 
  filter(!is.na(mhi),
         !is.na(mhv))
```


The resulting dataset has `r nrow(data)` rows by `r ncol(data)` columns.

## conduct a correlation analysis

```{r, fig.height = 12, fig.width = 10}
M = cor(data[,-1])
# ?corrplot
corrplot(M, type = c("upper"), order = c("hclust"))
```

There are several race/ethnicity variables that are correlated with each other and economic- and education-related variables.

It will be easier to examine highly correlated variables using a cutoff to view those correlations.

We start with a cutoff of Pearson's R-squared $\ge$ 0.7, which results in the following:

```{r}
output = vector(mode = "list", length = ncol(M))
rname = NULL
cname = NULL
for(i in 1:nrow(M)) {
  for(j in 1:ncol(M)) {
    rname[[i]] = dimnames(M)[[c(1, i)]]
    cname[[j]] = dimnames(M)[[c(2, j)]]
    output[[c(i, j)]] = tibble_row(i, j, rname[[i]], cname[[j]], M[[i, j]])
    # output[[i]] = bind_rows(output[[c(i, j)]])
    # output[[i]] = bind_rows(output[[c(i,j)]])
    # res[[c(i, j)]] = c(i, j, rname[[i]], cname[[j]], output[[c(i, j)]])
  }
  # output[[i]] = tibble(output[[c(i, j)]])
  # res = c(i, j, rname, cname)
  # res = bind_rows(res)
}
# head(output)
# union(output[[1]])
```

```{r, result = "hide"}

matrify = function(pdata = output) {
  # for(i in 1:46) {
  #   result[[i]] = vector(mode = "list", length = 46)
  # }
  for(i in 1:ncol(M)) {
    # for(j in 1:46) {
      result[[i]] = tibble(pdata[[c(i, i)]])
  }
  result = bind_rows(result)
  return(result)
    # result[[i]] = bind_rows(result[[c(i, j)]])
}
  # result[[i]] = bind_rows(result[[c(i, j)]])
#   return(result)
# }


# matrify()
```

# Matrify 2

```{r}
matrify2 = function(pdata = output) {
  result = NULL
  for(i in 1:length(pdata)) {
    # for(j in 1:46) {
      result[[i]] = tibble(pdata[[i]])
  }
  result = bind_rows(result)
  return(result)
    # result[[i]] = bind_rows(result[[c(i, j)]])
}

df2 = vector(mode = "list", length = ncol(M))
for(j in 1:ncol(M)) {
  df2[[j]] = matrify2(pdata = output[[j]])
}

head(df2)
```

```{r}
union_df = bind_rows(df2) %>% 
  rename("r2_value" = "M[[i, j]]",
         "column" = "cname[[j]]",
         "row" = "rname[[i]]")
```


```{r}
high_df = union_df %>% 
  filter(r2_value >= 0.7,
         r2_value != 1,
         i < j)

high_df
```


```{r}
very_high_df = union_df %>% 
  filter(r2_value >= 0.8,
         r2_value != 1,
         i < j)
very_high_df
```


# prcomp

```{r}
pca1 = prcomp(data[,-1], center = TRUE, scale = TRUE)
fviz_eig(pca1)

pca10 = principal(data[,-1], nfactors = 2, rotate = "varimax")
print(pca10)
fa.parallel(data[,-1])
```

```{r}
a = fviz_contrib(pca1, choice = "var", axes = 1) 
b = fviz_contrib(pca1, choice = "var", axes = 2) 
grid.arrange(a, b, nrow = 2)
```

# Center and Scale (confirm data) - PCA


```{r, fig.height = 12, fig.width = 14}
biplot(pca10)
```

## psych package

```{r}
# ?principal
fit = principal(M, nfactors = 6, rotate = "promax")
fit
```

### iclust in psych

```{r, fig.height = 16, fig.width = 14}
# ?iclust
# pdf()
fit2 = iclust(M, plot = TRUE)
# dev.off()
fit2
```


# Clustering

Further  methods in unsupervised learning

```{r}
# ?fviz_nbclust
fviz_nbclust(scale(data[,-1]),
             FUNcluster = kmeans,
             method = "silhouette")
```


```{r}
set.seed(719)
km <- kmeans(scale(data[,-1]), centers = 2)
# km2 <- kmeans(data[,-1], centers = 4)
# ?kmeans
# km2$cluster
# dimnames(km$cluster)[[2]]
# length(km$cluster)
# km$centers[[129]]
# str(km)
# km
```


```{r}
output2 = NULL
for(i in 1:length(km$cluster)) {
  output2[[i]] = tibble(km$cluster[[i]])
}

output2_df = bind_rows(output2) %>% 
  rename("cluster" = "km$cluster[[i]]")

# str(output2_df)
```


```{r}
data %>% 
  add_column(cluster = pull(output2_df, cluster)) %>% 
  select(cluster, geographic_area_name) %>% 
  arrange(cluster)
```

```{r, fig.height = 12, fig.width = 14}
# ?fviz_cluster
km_vis <- fviz_cluster(list(data = scale(data[,-1]), cluster = km$cluster),
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 10) + 
  labs(title = "K-means") 

km_vis
```

# Hierarchical Clustering

```{r}
hc.complete <- hclust(dist(scale(data[,-1])), method = "complete")

fviz_dend(hc.complete, k = 2,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 2)
# str(ind4.complete)
# names(ind4.complete)
# 
# # Who are in the fourth cluster?
# ind4.complete %>% 
#   filter()
```

## further clustering techniques

```{r, include = FALSE, eval = FALSE}
hc.average <- hclust(dist(centered_x), method = "average")

fviz_dend(hc.average, k = 3,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 3)

# Who are in the fourth cluster?
# dat[ind4.complete == 3,]
```


```{r, include = FALSE, eval = FALSE}
hc.single <- hclust(dist(data[,-1]), method = "single")

fviz_dend(hc.single, k = 3,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 3)

# Who are in the fourth cluster?
# dat[ind4.complete == 3,]
```


```{r, include = FALSE, eval = FALSE}
hc.centroid <- hclust(dist(data[,-1]), method = "centroid")

fviz_dend(hc.centroid, k = 3,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 3)

# Who are in the fourth cluster?
# dat[ind4.complete == 3,]

```


