---
title: "20201227-millRiver_covidStudy_jsg2145"
author: "Jared Garfinkel"
date: "12/27/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(readxl)
library(corrplot)
library(factoextra)
library(gridExtra)
library(psych)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 12, 
  fig.height = 14,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Part 1

## Read in data

## Keep the outcome (cases) separate

```{r}
cases = readxl::read_excel("./data/NassauCountyCOVID.xlsx") %>% 
  janitor::clean_names() %>% 
  select(namelsad, pos_12102020)

cases
```

```{r}
df = readxl::read_excel("./data/DianaData_11_25_20.xlsx") %>% 
  janitor::clean_names()

# observe dataframe

skimr::skim(df)

# missing data in "median rooms", "mhv", and "mhi"

df %>% 
  filter(is.na(mhi))
```

The dataset is `r nrow(df)` rows by `r ncol(df)` columns.

There are several ways of handling missing data, including available case analysis, complete case analysis, and multiple imputation.

In this case, we will simply remove the "median rooms" variable as it is not expected to contribute meaningfully to the correlation analysis.

The cases missing "mhv" (Median Home Value) and "mhi" (Median Household Income) values are removed for the correlation analysis.

This results in a loss of efficiency of 5/134 = `r round(5/134, 3)*100` percent.

```{r}
data = df %>% 
  dplyr::select(-median_rooms) %>% 
  filter(!is.na(mhi),
         !is.na(mhv))
```


The resulting dataset has `r nrow(data)` rows by `r ncol(data)` columns.

## conduct a correlation analysis

```{r, fig.height = 12, fig.width = 10}
M = cor(data[,-1])
# ?corrplot
corrplot(M, type = c("upper"), order = c("hclust"))
```

There are several race/ethnicity variables that are correlated with each other and economic- and education-related variables.

It will be easier to examine highly correlated variables using a cutoff to view those correlations.

We start with a cutoff of Pearson's R-squared $\ge$ 0.7, which results in the following:

```{r, include = FALSE, eval = FALSE}
# row.names(M)[[4]]
M[[c(4,1)]]
# str(M)
# attributes(M)
# dimnames(M)[[1]]
length(M)
nrow(M)
M[[46]]
dimnames(M)[[c(1, 46)]]
```


```{r}
output = vector(mode = "list", length = 46)
rname = NULL
cname = NULL
for(i in 1:nrow(M)) {
  for(j in 1:ncol(M)) {
    rname[[i]] = dimnames(M)[[c(1, i)]]
    cname[[j]] = dimnames(M)[[c(2, j)]]
    output[[c(i, j)]] = tibble_row(i, j, rname[[i]], cname[[j]], M[[i, j]])
    # output[[i]] = bind_rows(output[[c(i, j)]])
    # output[[i]] = bind_rows(output[[c(i,j)]])
    # res[[c(i, j)]] = c(i, j, rname[[i]], cname[[j]], output[[c(i, j)]])
  }
  # output[[i]] = tibble(output[[c(i, j)]])
  # res = c(i, j, rname, cname)
  # res = bind_rows(res)
}
# head(output)
# union(output[[1]])
```

```{r, result = "hide"}

matrify = function(pdata = output) {
  # for(i in 1:46) {
  #   result[[i]] = vector(mode = "list", length = 46)
  # }
  for(i in 1:46) {
    # for(j in 1:46) {
      result[[i]] = tibble(pdata[[c(i, i)]])
  }
  result = bind_rows(result)
  return(result)
    # result[[i]] = bind_rows(result[[c(i, j)]])
}
  # result[[i]] = bind_rows(result[[c(i, j)]])
#   return(result)
# }


# matrify()
```

# Matrify 2

```{r}
matrify2 = function(pdata = output) {
  result = NULL
  for(i in 1:length(pdata)) {
    # for(j in 1:46) {
      result[[i]] = tibble(pdata[[i]])
  }
  result = bind_rows(result)
  return(result)
    # result[[i]] = bind_rows(result[[c(i, j)]])
}

df2 = vector(mode = "list", length = 46)
for(j in 1:46) {
  df2[[j]] = matrify2(pdata = output[[j]])
}

head(df2)
```

```{r}
union_df = bind_rows(df2) %>% 
  rename("value" = "M[[i, j]]",
         "column" = "cname[[j]]",
         "row" = "rname[[i]]")
```


```{r}
high_df = union_df %>% 
  filter(value >= 0.7,
         value != 1,
         i < j)

high_df
```

There is high correlation (Pearson's R-squared > 0.7) in `r nrow(high_df)` pairs of variables.

The percentage of non-white residents is highly correlated with 6 other variables, including the percent of residents who are black, hispanic, other religion, foreign born, non-English speaking and who did not graduate high school.

The percentage of hispanic residents is highly correlated with 3 variables: the percent of residents who are essential workers, who have no health insurance, and who did not graduate high school.

The percentage of residents who did not graduate high school is highly correlated with 7 variables: the percent of residents who are hispanic, who are essential workers, who are on SNAP, who have no health insurance, who earn less than the poverty level, who are other religion, and who are non-white.

```{r, include = FALSE, eval = FALSE}
high_df %>% 
  mutate(n = n()) %>% 
  group_by(column, n) %>% 
  summarize(count = n()) %>% 
  mutate(p = count/n)
  # ungroup() %>% 
  # mutate(total = sum(p))
```

```{r}
very_high_df = union_df %>% 
  filter(value >= 0.8,
         value != 1,
         i < j)
very_high_df
```

Furthermore, there were 6 pairs of variables with Pearson R-squared values above 0.8 including: (1) the total population and total cases of COVID-19 (Pearson R-squared `r very_high_df %>% filter(i==3) %>% pull(value)`), (2) the percent of non-white residents and the percent of Black residents (Pearson R-squared `r very_high_df %>% filter(i==8) %>% pull(value)`), (3) the percent of children and percent of family with kids (Pearson R-squared `r very_high_df %>% filter(i==10) %>% pull(value)`), (4) the percent of poverty and the percent of children in poverty (Pearson R-squared `r very_high_df %>% filter(i==19) %>% pull(value)`), (5) the percent of foreign born residents and the percent of non-English speaking residents (Pearson R-squared `r very_high_df %>% filter(i==33) %>% pull(value)`), and (6) the percent of residents who did not graduate high school and the percent who are hispanic (Pearson R-squared `r very_high_df %>% filter(i==41) %>% pull(value)`).

```{r, include = FALSE, eval = FALSE}
for(j in 1:46) {
  matrify(output[[j]])
}
```


```{r,include = FALSE,result = "hide", eval = FALSE}
head(result)
head(output)
tbl = tibble(output[[1]])
```

```{r, include = FALSE, eval = FALSE}
result = NULL
for(i in 1:46){
#   result[[i]] = tibble(output[[1, i]])
# }
#   # for(j in 1:46) {
#   #   result[[i]] = tibble(output[[i]][[c(i, j)]])
#   # }
  result[[i]] = tibble(output[[c(1, i)]])
}
# head(result)
bind_rows(result)
```

```{r, include = FALSE, eval = FALSE}
str(head(output))
output
```

```{r, include = FALSE, eval = FALSE}
str(output[[c(1, 1)]])
result = vector(mode = "list", length = 1)
for (i in 1:5) {
  result[[i]] = bind_cols(output[[c(1, 1)]][[i]])
}
result
output[[c(1, 1)]]
# ?tibble
# length(output)
# output[[3]][[46]]
# output[[c(46, 46)]]
# ?map
result = vector(mode = "list", length = 46)
for(i in 1:length(output[[i]])) {
  
#   result[[i]] = map(output[[i]], ~bind_rows(.))
# }
  for(j in 1:length(output[[i]])) {
    result[[c(i, j)]] = map(output[[c(i, j)]], bind_rows)
  }
}
head(output)
# ?bind_cols
head(str(head(output)))
head(output)
output[[c(i, j)]]
output = bind_rows
# ?
head(output)
#   rname[[i]] = row.names(M)[[i]]
#   cname[[i]] = col
#   output[[i]] = M[[i]] %>% 
#     filter()
# }
# M %>% 
#   filter()
```

# prcomp

```{r}
pca = prcomp(data[,-1])
fviz_eig(pca)
  # ?screeplot
```

```{r}
a = fviz_contrib(pca, choice = "var", axes = 1) 
b = fviz_contrib(pca, choice = "var", axes = 2) 
grid.arrange(a, b, nrow = 2)
```

The PCA isn't working because of the size of the mhv and mhi variables. A centered scaled dataset is needed.

```{r}
pca1 = prcomp(data[,-1], scale = TRUE)
fviz_eig(pca1)
```

```{r}
a = fviz_contrib(pca1, choice = "var", axes = 1) 
b = fviz_contrib(pca1, choice = "var", axes = 2) 
grid.arrange(a, b, nrow = 2)
```

# Part 2

## read in data

```{r, results = "hide"}
cs_data = readxl::read_excel("./data/CR_Index_Jared_12_22_20.xlsx", sheet = "Score") %>% 
  janitor::clean_names()

skimr::skim(cs_data)
```

There is no missing data.

## check a correlation analysis

```{r}
cs_matrify = function(pdata = cs_output) {
  result = vector(mode = "list", length = 35)
  for(i in 1:35) {
      result[[i]] = tibble(pdata[[i]])
  }
  result = bind_rows(result)
  return(result)
}
```


```{r}
M2 = cor(cs_data[,-1])

# length(M2)
# ncol(M2)
# length(cs_output)

cs_output = vector(mode = "list", length = length(M2))
rname = NULL
cname = NULL
for(i in 1:nrow(M2)) {
  for(j in 1:ncol(M2)) {
    rname[[i]] = dimnames(M2)[[c(1, i)]]
    cname[[j]] = dimnames(M2)[[c(2, j)]]
    cs_output[[c(i, j)]] = tibble_row(i, j, rname[[i]], cname[[j]], M2[[i, j]])
  }
}

# matrify = function(pdata = cs_output) {
#   for(i in 1:length(M2)) {
#       result[[i]] = tibble(pdata[[c(i, i)]])
#   }
#   result = bind_rows(result)
#   return(result)
# }
# 
# matrify()

# cs_output[[35]]

cs_df = vector(mode = "list", length = 35)
for(j in 1:35) {
  cs_df[[j]] = cs_matrify(pdata = cs_output[[j]])
}

# head(cs_df)

union_cs_df = bind_rows(cs_df) %>% 
  rename("value" = "M2[[i, j]]",
         "column" = "cname[[j]]",
         "row" = "rname[[i]]")
```

```{r}
high_df = union_cs_df %>% 
  filter(value >= 0.7,
         value != 1,
         i < j)

high_df

very_high_df = union_cs_df %>% 
  filter(value >= 0.8,
         value != 1,
         i < j)
very_high_df
```

```{r}
# ?prcomp
pca = prcomp(cs_data[,-1])
fviz_eig(pca)
# ?fviz_eig
```


```{r}
a = fviz_contrib(pca, choice = "var", axes = 1) 
b = fviz_contrib(pca, choice = "var", axes = 2) 
c = fviz_contrib(pca, choice = "var", axes = 3) 
grid.arrange(a, b, c, nrow = 3)
```

# Center and Scale (confirm data) - PCA

```{r}
centered_x = scale(data[,-1])
rownames(centered_x) = data$geographic_area_name
```

```{r}
# cs_matrify = function(pdata = cs_output) {
#   result = vector(mode = "list", length = 35)
#   for(i in 1:35) {
#       result[[i]] = tibble(pdata[[i]])
#   }
#   result = bind_rows(result)
#   return(result)
# }
```

```{r}
centered_M = cor(centered_x)
centered_output = vector(mode = "list", length = length(centered_M))
rname = NULL
cname = NULL
for(i in 1:nrow(centered_M)) {
  for(j in 1:ncol(centered_M)) {
    rname[[i]] = dimnames(centered_M)[[c(1, i)]]
    cname[[j]] = dimnames(centered_M)[[c(2, j)]]
    centered_output[[c(i, j)]] = tibble_row(i, j, rname[[i]], cname[[j]], centered_M[[i, j]])
  }
}
```

```{r}
centered_df = vector(mode = "list", length = 35)
for(j in 1:35) {
  centered_df[[j]] = cs_matrify(pdata = centered_output[[j]])
}
```

```{r}
union_centered_df = bind_rows(centered_df) %>% 
  rename("value" = "centered_M[[i, j]]",
         "column" = "cname[[j]]",
         "row" = "rname[[i]]")
```

```{r}
high_df = union_centered_df %>% 
  filter(value >= 0.7,
         value != 1,
         i < j)

high_df

very_high_df = union_centered_df %>% 
  filter(value >= 0.8,
         value != 1,
         i < j)
very_high_df
```

There are fewer highly correlated variables using the method of "scale" in R than the dataset provided.

```{r, fig.height = 12, fig.width = 14}
pca2 = prcomp(centered_x)
fviz_eig(pca2)
biplot(pca2)
```

```{r}
a = fviz_contrib(pca, choice = "var", axes = 1) 
b = fviz_contrib(pca, choice = "var", axes = 2) 
grid.arrange(a, b, nrow = 2)
```

These results are similar to those using the option "scale" in the "prcomp" function as earlier. This makes sense, since the option "scale" sends the data to the "scale" function.

## psych package

```{r}
# ?principal
fit = principal(M2, nfactors = 4, rotate = "promax")
fit
```

### iclust in psych

```{r, fig.height = 16, fig.width = 14}
# ?iclust
# pdf()
fit2 = iclust(M2, plot = TRUE)
# dev.off()
fit2
```


# Clustering

Further  methods in unsupervised learning

```{r}
fviz_nbclust(centered_x,
             FUNcluster = kmeans,
             method = "silhouette")
```


```{r}
set.seed(719)
km <- kmeans(centered_x, centers = 3)
# km2 <- kmeans(data[,-1], centers = 4)
# ?kmeans
# km2$cluster
# dimnames(km$cluster)[[2]]
# length(km$cluster)
# km$centers[[129]]
# str(km)
# km
```


```{r}
output2 = NULL
for(i in 1:length(km$cluster)) {
  output2[[i]] = tibble(km$cluster[[i]])
}

output2_df = bind_rows(output2) %>% 
  rename("cluster" = "km$cluster[[i]]")

# str(output2_df)
```


```{r}
data %>% 
  add_column(cluster = pull(output2_df, cluster)) %>% 
  select(cluster, geographic_area_name) %>% 
  arrange(cluster)
```

```{r, fig.height = 12, fig.width = 14}
# ?fviz_cluster
km_vis <- fviz_cluster(list(data = centered_x, cluster = km$cluster),
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 10) + 
  labs(title = "K-means") 

km_vis
```

# Hierarchical Clustering

```{r}
hc.complete <- hclust(dist(centered_x), method = "complete")

fviz_dend(hc.complete, k = 3,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 3)
# str(ind4.complete)
# names(ind4.complete)
# 
# # Who are in the fourth cluster?
# ind4.complete %>% 
#   filter()
```


```{r}
hc.average <- hclust(dist(centered_x), method = "average")

fviz_dend(hc.average, k = 3,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 3)

# Who are in the fourth cluster?
# dat[ind4.complete == 3,]
```


```{r}
hc.single <- hclust(dist(data[,-1]), method = "single")

fviz_dend(hc.single, k = 3,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 3)

# Who are in the fourth cluster?
# dat[ind4.complete == 3,]
```


```{r}
hc.centroid <- hclust(dist(data[,-1]), method = "centroid")

fviz_dend(hc.centroid, k = 3,        
          cex = 0.5, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# ?fviz_dend

ind4.complete <- cutree(hc.complete, 3)

# Who are in the fourth cluster?
# dat[ind4.complete == 3,]

```


